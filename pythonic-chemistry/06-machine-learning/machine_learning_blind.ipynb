{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. Over the last couple of decades, the technological advances in storage and processing power have enabled some innovative products based on machine learning, such as Netflix's recommendation engine and self-driving cars.\n",
    "\n",
    "Machine learning is an important component of the growing field of data science. Through the use of statistical methods, algorithms are trained to make classifications or predictins, and to uncover key insights in data mining projects. These insights subsequently drive decision making within applications and businesses, ideally impacting key growth metrics. As big data continus to expand and grow, the market demand for data scientists will increase. They will be required to help identify the most relevant business questions and the data to answer them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./deep_learning.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 How Machine Learning Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break machin learning algorithms into three main parts:\n",
    "\n",
    "- `A Decision Process:` In general, machine learning algorithms are used to make a prediction or classification. Based on some input data, which can be labeled or unlabeled, your algorithm will produce an estimate about a pattern in the data. \n",
    "\n",
    "- `An Error Function:` An error function evaluates the prediction of the model. If there are known examples, an error functon can make a comparison to assess the accuracy of the model.\n",
    "\n",
    "- `A Model Optimization Process`: If the model can fit better the data points in the training set, then weights are adjusted to reduce the discrepancy between the known example and the model estimate. The algorithm will repat this \"evaluate and optimize\" process, updating weigths autonomously until a threshold of accuracy has been met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Machine Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models fall into three primary categories:\n",
    "\n",
    "- **Supervised Machine Learning**: Is defined by its use of labeled datasets to train algorithms to classify data or predict outcomes accurately. As input data is fed into the model, the model adjusts its weights until it has been fitted appropiately. This occurs as part of the cross validation process to ensure that the model avoids **overfitting** or **underfitting**. Supervised learning helps organizations solve a variety of real-world problems at scale, such as classifyting spam in a separate folder from your inbox. Some methods used in supervised learning include neural networks, na√Øve bayes, linear regression, logistic regression, radom forest and support vector machine (SVM).\n",
    "\n",
    "- **Unsupervised Machine Learning**: Is the use of machine learning algorithms to analyze and cluster unlabeled datasets. These algorithms discover hidden patterns or data groupings without the need for human intervation. This method's ability to discover similarities and differences in information make it ideal for exploratory data analysis cross-selling strategies, customer segmentation, and image and pattern recognition. It's also used to reduce the number of features in a model through the process of dimensionality reduction. Principal component analysis (PCA) and singular value decomposition (SVD) are two common approaches for this. Other algorithms used in unsupervised learning include neural networks, k-means clustering and probabilistic clustering methods.\n",
    "\n",
    "- **Semi-supervised learning**: These methods offers a happy medium between supervised and unsupervised learning. During training, its uses a smaller labeled data set to guide classification and feature extraction from a larger, unlabeled dataset. Semi-supervised learning can solve the problem of not having enough labeled data for a supervised learning algorithm. It also helps if it's too costly to label enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with `supervised learning` with a typical dataset, the `iris` dataset. It is a small classic dataset from Fisher (1936). One of the earliest known datasets used for evaluating classification methods.\n",
    "\n",
    "The dataset contains 3 different classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are not linearly separable from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to lead the csv file with pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/iris.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get some data out of the csv file without any ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool! We can see that we have 4 features sepal length, sepal width, petal length, petal width. Then, there are no NaNs in the dataset so, we don't need to implement any strategy to remove NaNs. Finally the 3 classes are **balanced** so we don't need to worry about data **imbalance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some quick visuals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.pairplot(df, kind=\"reg\", hue=\"species\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, we need to preprocess a bit our data so the categorical targets that we have as strings \"Iris-setosa\" to change it to numerical values that the ML model will be able to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_maker = LabelEncoder()\n",
    "\n",
    "df[\"species\"] = lb_maker.fit_transform(df[\"species\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explore this problem as a `classification problem` where the model will be able to determine to which class the data point is closest.\n",
    "\n",
    "First we need to split our dataset into `train` and `test`, so then we will be able to train our model and validate its performance on data that haven't seen during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st split between features (input) and target (species)\n",
    "x = np.array(df.iloc[:, 0:4])\n",
    "y = np.array(df.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import a helper method that will allow us to split between train/test in a random way\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first classification model we will use the `KNeighborsClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the method\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the model\n",
    "knn = KNeighborsClassifier(n_neighbors=2, algorithm=\"auto\", leaf_size=30, metric=\"minkowski\", metric_params=None, p=2, weights=\"uniform\")\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict the values from the test set\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate its performance by comparing the original labels to the predicted ones\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"The KKN model accuracy is {accuracy_score(y_pred, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool! we got so far a 96% of the test set correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1** Go back to the previous example a tune a bit the number of neighbours to get a better performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2** Do the same as above but with a new model `DecisionTreeClassifier` and compare their performances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn tree family\n",
    "\n",
    "# Initialize model\n",
    "\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict the values from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Decision Tree Classifier accuracy is {accuracy_score(y_pred, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cool thing about decision trees is that we can explore the actual tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `regression problem` is a different way to understand the task that we want to streamline with ML. Regression algorithms predict a continuous value based on the input variables. The main goal of regression problems is to estimate a mapping function based on the input and output variable. If tour target vriable is quantity like inconme, scores, height or weight, then you should use the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import a helper method that will allow us to split between train/test in a random way\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LogisticRegression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict on the test set\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The LogisticRegressor accuracy is {accuracy_score(y_pred, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Model optimization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are parameters that are not direclty learnt within estimators. In scikit-learn they are passed as argumenets to the constructor of the estimator classes. The grid search by `GridSearchCV` exhaustively generates from a grid of parameter values specified within the **param_grid** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import a helper method that will allow us to split between train/test in a random way\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set a grid as a dictionary\n",
    "grid = {\"n_neighbors\": np.arange(1, 50)}\n",
    "\n",
    "# KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Grid search\n",
    "knn_cv = GridSearchCV(knn, grid, cv=10)\n",
    "\n",
    "# Train\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tunned hyperparameter K:\", knn_cv.best_params_)\n",
    "print(\"Best score: \", knn_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is never enough data to train your model, removing a part of it for validation poses a problem  of underfitting. By reducing the training data, we risk losing important patterns/trends in data set, which in turn increases error induced by bias. So, what we requiere is a method that provides ample daa for training the model and also leaves ample data for validation. K-Fold cross validation does exactly that.\n",
    "\n",
    "In `K-Fold` cross validation, the data is divided into k subsets. Now holdout method is repeated k times, such that each time, one of the k subsets is used as the test set/validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total evectiveness of your model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchaning the training and test sets also adds to the effectiveness of this method. As a general rule and empirical evidence, K=5 or K=10 is generally preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./kfold_cross_validation.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import a helper method that will allow us to split between train/test in a random way\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "# Initialize the model\n",
    "lr = LogisticRegression(max_iter=300)\n",
    "\n",
    "scores = []\n",
    "kfold = KFold(n_splits=10)\n",
    "for idx, (train_index, test_index) in enumerate(kfold.split(x)):\n",
    "    # Lower case to differenciate between X and x\n",
    "    x_train, x_test, y_train, y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    # Store scores\n",
    "    scores.append(lr.score(x_test, y_test))\n",
    "\n",
    "    print(f\"Kfold: {idx} has a score of {lr.score(x_test, y_test)}\")\n",
    "\n",
    "# The scores' mean\n",
    "print(f\"The score's mean is {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonic-chemistry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
